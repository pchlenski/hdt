{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# hSVM and logistic regression\n",
    "> Benchmarking two more hyperbolic classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# hSVM and hMLR benchmark:\n",
    "\n",
    "This code should be run using the `hsvm` conda environment instead of the `hdt` conda environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/phil/hdt/notebooks/../hsvm/htools.py:29: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n",
      "  @jit(parallel=True)\n",
      "/home/phil/hdt/notebooks/../hsvm/htools.py:108: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n",
      "  @jit(parallel=True)\n",
      "INFO:root:Using numpy backend\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# hSVM stuff\n",
    "sys.path.append(\"../hsvm\")\n",
    "from hsvm import LinearHSVM\n",
    "\n",
    "# hLR stuff\n",
    "sys.path.append(\"../HyperbolicCV/code\")\n",
    "from lib.lorentz.layers.LMLR import LorentzMLR\n",
    "from lib.lorentz.manifold import CustomLorentz\n",
    "\n",
    "import torch\n",
    "\n",
    "# For benchmarking\n",
    "# from hyperdt.toy_data import wrapped_normal_mixture\n",
    "sys.path.append(\"../HoroRF\")\n",
    "from datasets.gaussian import get_training_data, get_testing_data\n",
    "\n",
    "# from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "from sklearn.metrics import f1_score\n",
    "import time\n",
    "from tqdm import tqdm_notebook as tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suppress UserWarning from sklearn and FutureWarning from numba\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train hMLR function\n",
    "\n",
    "\n",
    "def train_hmlr(X, y, steps=1000):\n",
    "    # Init class...\n",
    "    hmlr = LorentzMLR(num_features=X.shape[1], num_classes=2, manifold=CustomLorentz())\n",
    "\n",
    "    # hMLR outputs logits; labels are {0, 1}\n",
    "    opt = torch.optim.Adam(hmlr.parameters(), lr=0.01)\n",
    "    loss_fn = torch.nn.BCEWithLogitsLoss()\n",
    "\n",
    "    for _ in range(steps):\n",
    "        opt.zero_grad()\n",
    "        logits = hmlr(X)\n",
    "        loss = loss_fn(logits[:, 1], y)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "\n",
    "    return hmlr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dea95190758c4ca7af0e4d1ea10420d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=400), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "results = pd.DataFrame(columns=[\"seed\", \"n_dim\", \"model\", \"f1_score\", \"time\"])\n",
    "\n",
    "seeds = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
    "dims = [2, 4, 8, 16]\n",
    "my_tqdm = tqdm(total=len(seeds) * len(dims) * 2 * 5)\n",
    "\n",
    "for n_dim in dims:\n",
    "    for seed in seeds:\n",
    "        # print(n_dim, seed)\n",
    "        my_tqdm.set_description(f\"{n_dim}, {seed}\")\n",
    "        X, y = get_training_data(class_label=n_dim, seed=seed, num_samples=int(800 / 0.8), convert_to_poincare=False)\n",
    "\n",
    "        # Both models like hyperboloids, so this is easy\n",
    "        folds = KFold(n_splits=5, shuffle=True, random_state=seed)\n",
    "\n",
    "        for train_index, test_index in folds.split(X):\n",
    "            X_train, X_test = X[train_index], X[test_index]\n",
    "            y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "            X_train_torch = torch.tensor(X_train, dtype=torch.float)\n",
    "            y_train_torch = torch.tensor(y_train, dtype=torch.float)\n",
    "            X_test_torch = torch.tensor(X_test, dtype=torch.float)\n",
    "            y_test_torch = torch.tensor(y_test, dtype=torch.float)\n",
    "\n",
    "            # hSVM\n",
    "            t1 = time.time()\n",
    "            hsvm = LinearHSVM()\n",
    "            hsvm.fit(X_train, y_train)\n",
    "            y_pred = hsvm.predict(X_test)\n",
    "            t2 = time.time()\n",
    "            results.loc[len(results)] = [seed, n_dim, \"hSVM\", f1_score(y_test, y_pred, average=\"micro\"), t2 - t1]\n",
    "            my_tqdm.update()\n",
    "\n",
    "            # hMLR\n",
    "            t1 = time.time()\n",
    "            hmlr = train_hmlr(X_train_torch, y_train_torch)\n",
    "            y_pred = hmlr(X_test_torch).argmax(dim=1).clone().detach().numpy()\n",
    "            t2 = time.time()\n",
    "            results.loc[len(results)] = [seed, n_dim, \"hMLR\", f1_score(y_test, y_pred, average=\"micro\"), t2 - t1]\n",
    "            my_tqdm.update()\n",
    "\n",
    "results.to_csv(\"../data/processed/hsvm_hmlr_results.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "model  n_dim\n",
       "hMLR   2        0.904000\n",
       "       4        0.991375\n",
       "       8        0.999875\n",
       "       16       1.000000\n",
       "hSVM   2        0.514875\n",
       "       4        0.435000\n",
       "       8        0.464000\n",
       "       16       0.460000\n",
       "Name: f1_score, dtype: float64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.groupby([\"model\", \"n_dim\"]).mean()[\"f1_score\"] * 100\n",
    "\n",
    "# Compare to horoDT: \n",
    "#   2   91.88\n",
    "#   4   99.30\n",
    "#   8   99.96\n",
    "#  16   100.00"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hsvm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
